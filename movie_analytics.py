# -*- coding: utf-8 -*-
"""Movie Analytics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12-QWDc0mP6W2zAPsNNTDvRY7Wj3cvUEW
"""

import pandas as pd
from google.colab import drive
drive.mount('/content/drive')

# Define the file paths
movies_path = '/content/drive/MyDrive/Dataset/Movie Analytics/movies.dat'
ratings_path = '/content/drive/MyDrive/Dataset/Movie Analytics/ratings.dat'
users_path = '/content/drive/MyDrive/Dataset/Movie Analytics/users.dat'

# Load the data with the specified encoding
movies = pd.read_csv(movies_path, delimiter='::', engine='python', header=None, names=['MovieID', 'Title', 'Genres'], encoding='ISO-8859-1')
ratings = pd.read_csv(ratings_path, delimiter='::', engine='python', header=None, names=['UserID', 'MovieID', 'Rating', 'Timestamp'], encoding='ISO-8859-1')
users = pd.read_csv(users_path, delimiter='::', engine='python', header=None, names=['UserID', 'Gender', 'Age', 'Occupation', 'Zip-code'], encoding='ISO-8859-1')

# Display the first few rows of each dataframe
print("Movies Data:")
print(movies.head())

print("\nRatings Data:")
print(ratings.head())

print("\nUsers Data:")
print(users.head())

"""#**Install PySpark**"""

!pip install pyspark

"""**Set Up Spark Session**"""

from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("MovieLensAnalysis") \
    .getOrCreate()

"""**Convert Pandas DataFrames to Spark DataFrames**"""

movies_spark_df = spark.createDataFrame(movies)
ratings_spark_df = spark.createDataFrame(ratings)
users_spark_df = spark.createDataFrame(users)

"""**Register DataFrames as temporary views for Spark SQL**"""

movies_spark_df.createOrReplaceTempView("movies")
ratings_spark_df.createOrReplaceTempView("ratings")
users_spark_df.createOrReplaceTempView("users")

"""# **Analytical Queries:**

1. Top 10 Most Viewed Movies
"""

top_viewed_movies = spark.sql("""
    SELECT m.Title, COUNT(r.MovieID) as view_count
    FROM ratings r
    JOIN movies m ON r.MovieID = m.MovieID
    GROUP BY m.Title
    ORDER BY view_count DESC
    LIMIT 10
""")
top_viewed_movies.show()

"""2. Distinct List of Genres"""

import pyspark.sql.functions as F

# Ensure genres are properly split and exploded
movies_spark_df = movies_spark_df.withColumn("Genre", F.explode(F.split(F.col("Genres"), "\\|")))

# Register the updated DataFrame as a temporary view
movies_spark_df.createOrReplaceTempView("movies")

# Extract distinct genres
distinct_genres = spark.sql("""
    SELECT DISTINCT Genre
    FROM movies
""")

# Show distinct genres
distinct_genres.show(truncate=False)

"""3. Number of Movies for Each Genre"""

movies_per_rating = spark.sql("""
    SELECT Rating, COUNT(*) as count
    FROM ratings
    GROUP BY Rating
    ORDER BY count DESC
""")
movies_per_rating.show()

"""4. Number of Movies Starting with Numbers or Letters"""

users_per_movie = spark.sql("""
    SELECT MovieID, COUNT(DISTINCT UserID) as num_users
    FROM ratings
    GROUP BY MovieID
    ORDER BY num_users DESC
""")
users_per_movie.show()

"""5. List of Latest Released Movies"""

total_rating_per_movie = spark.sql("""
    SELECT MovieID, SUM(Rating) as total_rating
    FROM ratings
    GROUP BY MovieID
    ORDER BY total_rating DESC
""")
total_rating_per_movie.show()

"""# **Spark SQL**

1. Create tables for movies.dat, users.dat, and ratings.dat in Spark SQL
"""

from pyspark.sql import SparkSession

# Initialize Spark Session
spark = SparkSession.builder.appName("MovieLensAnalysis").getOrCreate()

# Load Data
movies_path = "/content/drive/MyDrive/Dataset/Movie Analytics/movies.dat"
ratings_path = "/content/drive/MyDrive/Dataset/Movie Analytics/ratings.dat"
users_path = "/content/drive/MyDrive/Dataset/Movie Analytics/users.dat"

# Load Movies Data
movies = spark.read.csv(movies_path, sep='::', header=False, inferSchema=True).toDF("MovieID", "Title", "Genres")
movies.createOrReplaceTempView("movies")

# Load Ratings Data
ratings = spark.read.csv(ratings_path, sep='::', header=False, inferSchema=True).toDF("UserID", "MovieID", "Rating", "Timestamp")
ratings.createOrReplaceTempView("ratings")

# Load Users Data
users = spark.read.csv(users_path, sep='::', header=False, inferSchema=True).toDF("UserID", "Gender", "Age", "Occupation", "Zip-code")
users.createOrReplaceTempView("users")

# Save tables in parquet format (optional)
movies.write.parquet("data/movies_table.parquet")
ratings.write.parquet("data/ratings_table.parquet")
users.write.parquet("data/users_table.parquet")

"""2. Find the list of the oldest released movies"""

from pyspark.sql.functions import regexp_extract

# Extract Year from the Title and create a new DataFrame
movies_with_year = movies_spark_df.withColumn("Year", regexp_extract("Title", r"\((\d{4})\)", 1))

# Register the new DataFrame with Year as a temporary SQL table
movies_with_year.createOrReplaceTempView("movies_with_year")

oldest_movies = spark.sql("""
    SELECT Title, Year
    FROM movies_with_year
    WHERE Year IS NOT NULL AND Year != ''
    ORDER BY Year ASC
    LIMIT 10
""")
oldest_movies.show()

"""3. How many movies are released each year?"""

from pyspark.sql.functions import regexp_extract

# Extract Year from Title and store it in a new DataFrame
movies_with_year = movies_spark_df.withColumn("Year", regexp_extract("Title", r"\((\d{4})\)", 1))

# Register this new DataFrame as a table
movies_with_year.createOrReplaceTempView("movies_with_year")

movies_per_year = spark.sql("""
    SELECT Year, COUNT(*) as MovieCount
    FROM movies_with_year
    WHERE Year IS NOT NULL AND Year != ''
    GROUP BY Year
    ORDER BY Year ASC
""")
movies_per_year.show()

"""4. How many movies are there for each rating?"""

movies_per_rating = spark.sql("""
    SELECT Rating, COUNT(*) as MovieCount
    FROM ratings
    GROUP BY Rating
    ORDER BY MovieCount DESC
""")
movies_per_rating.show()

"""5. How many users have rated each movie?"""

users_per_movie = spark.sql("""
    SELECT MovieID, COUNT(DISTINCT UserID) as UserCount
    FROM ratings
    GROUP BY MovieID
    ORDER BY UserCount DESC
""")
users_per_movie.show()

"""6. What is the total rating for each movie?"""

total_ratings_per_movie = spark.sql("""
    SELECT r.MovieID, m.Title, SUM(r.Rating) as TotalRating
    FROM ratings r
    JOIN movies m ON r.MovieID = m.MovieID
    GROUP BY r.MovieID, m.Title
    ORDER BY TotalRating DESC
""")
total_ratings_per_movie.show()

"""7. What is the average rating for each movie?"""

average_ratings_per_movie = spark.sql("""
    SELECT r.MovieID, m.Title, AVG(r.Rating) as AvgRating
    FROM ratings r
    JOIN movies m ON r.MovieID = m.MovieID
    GROUP BY r.MovieID, m.Title
    ORDER BY AvgRating DESC
""")
average_ratings_per_movie.show()

"""# **Spark Data Frames**

1. Prepare Movies Data: Extracting the Year and Genre from the Text
"""

movies_df = spark.read.csv(movies_path, sep='::', header=False, inferSchema=True)\
    .toDF('MovieID', 'Title', 'Genres')

# Extract year from title
movies_df = movies_df.withColumn('Year', F.regexp_extract(F.col('Title'), r'\((\d{4})\)$', 1).cast('int'))

# Split genres into an array
movies_df = movies_df.withColumn('Genres', F.split(F.col('Genres'), '\\|'))

movies_df.show(truncate=False)

"""2. Prepare Users Data: Loading a Double Delimited CSV File"""

users_df = spark.read.csv(users_path, sep='::', header=False, inferSchema=True)\
    .toDF('UserID', 'Gender', 'Age', 'Occupation', 'Zip-code')

users_df.show(truncate=False)

"""3. Prepare Ratings Data: Programmatically Specifying a Schema for the Data Frame"""

from pyspark.sql.types import StructType, StructField, IntegerType, LongType

# Define schema
schema = StructType([
    StructField("UserID", IntegerType(), True),
    StructField("MovieID", IntegerType(), True),
    StructField("Rating", IntegerType(), True),
    StructField("Timestamp", LongType(), True)
])

# Load ratings data
ratings_df = spark.read.csv(ratings_path, sep='::', header=False, schema=schema)

ratings_df.show(truncate=False)

"""5. Save Table without Defining DDL in Hive"""

# Assuming Hive is set up and configured
movies_df.write.saveAsTable("movies")

"""6. Broadcast Variable Example"""

from pyspark.sql import Row

# Create a broadcast variable
movie_titles = {row['MovieID']: row['Title'] for row in movies_df.collect()}
movie_titles_broadcast = spark.sparkContext.broadcast(movie_titles)

# Example of using the broadcast variable in a UDF
def get_movie_title(movie_id):
    return movie_titles_broadcast.value.get(movie_id, 'Unknown')

from pyspark.sql.functions import udf
get_movie_title_udf = udf(get_movie_title)

# Apply the UDF to add a new column with movie titles
ratings_with_titles_df = ratings_df.withColumn('Title', get_movie_title_udf(F.col('MovieID')))
ratings_with_titles_df.show()

"""7. Accumulator Example"""

# Example accumulator
accumulator = spark.sparkContext.accumulator(0)

def increment_accumulator(rating):
    global accumulator
    if rating > 4:
        accumulator += 1

# Apply the function to each row
ratings_df.foreach(lambda row: increment_accumulator(row['Rating']))

print(f"Number of high ratings: {accumulator.value}")